#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import locale
import shutil
import re
from datetime import datetime
from bs4 import BeautifulSoup
import argparse

# =========================
# CONFIG
# =========================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# Î¤Î¿ script Ï„ÏÎ­Ï‡ÎµÎ¹ Î¼Î­ÏƒÎ± Î±Ï€ÏŒ Ï„Î¿ New-html, Î¿Ï€ÏŒÏ„Îµ Î¿ ROOT ÎµÎ¯Î½Î±Î¹ Î¿ Î³Î¿Î½Î­Î±Ï‚
NEW_HTML_DIR = BASE_DIR 
ROOT_DIR = os.path.dirname(BASE_DIR)

ARCHIVES = {
    "el": os.path.join(ROOT_DIR, "sparkethos-archives-el.html"),
    "en": os.path.join(ROOT_DIR, "sparkethos-archives-en.html"),
}

# =========================
# HELPERS
# =========================
def log(msg):
    print(msg)

def extract_icon(title):
    """Î•Î¾Î¬Î³ÎµÎ¹ Ï„Î¿ emoji Î±Ï€ÏŒ Ï„Î¿Î½ Ï„Î¯Ï„Î»Î¿"""
    emoji_pattern = re.compile(r'[\U0001f300-\U0001f9ff\u2700-\u27bf]')
    match = emoji_pattern.search(title)
    return match.group() if match else "ğŸ“„"

def clean_title(title):
    """ÎšÎ±Î¸Î±ÏÎ¯Î¶ÎµÎ¹ Ï„Î¿Î½ Ï„Î¯Ï„Î»Î¿ Î±Ï€ÏŒ emojis"""
    emoji_pattern = re.compile(r'[\U0001f300-\U0001f9ff\u2700-\u27bf]')
    return emoji_pattern.sub('', title).strip()

def format_date(date_obj, lang):
    try:
        if lang == "el":
            locale.setlocale(locale.LC_TIME, "el_GR.UTF-8")
            return date_obj.strftime("%d %B %Y")
        else:
            locale.setlocale(locale.LC_TIME, "en_US.UTF-8")
            return date_obj.strftime("%B %d, %Y")
    except locale.Error:
        return date_obj.strftime("%Y-%m-%d")

def extract_html_metadata(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f, "html.parser")
    
    title = soup.title.get_text(strip=True) if soup.title else os.path.basename(file_path)
    keywords_tag = soup.find("meta", {"name": "keywords"})
    keywords = keywords_tag["content"] if keywords_tag else ""
    
    # Î Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¿ ÏŒÎ½Î¿Î¼Î± Ï„Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï… Î±Ï€ÏŒ Ï„Î¿ canonical Î® Ï„Î¿ Î¯Î´Î¹Î¿ Ï„Î¿ ÏŒÎ½Î¿Î¼Î± Î±ÏÏ‡ÎµÎ¯Î¿Ï…
    canonical = soup.find("link", {"rel": "canonical"})
    href = canonical["href"].split("/")[-1] if canonical else os.path.basename(file_path)
    lang = soup.html.get("lang", "en") if soup.html else "en"
    
    date_obj = datetime.today()
    json_ld_tag = soup.find("script", {"type": "application/ld+json"})
    if json_ld_tag:
        try:
            data = json.loads(json_ld_tag.string)
            date_str = data.get("datePublished")
            if date_str:
                date_obj = datetime.fromisoformat(date_str.split('T')[0])
        except Exception:
            pass
    return title, keywords, href, lang, date_obj

def load_archive_soup(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ {path}")
    
    with open(path, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f.read(), "html.parser")
    
    section = soup.find("section", id="articles")
    if not section:
        raise RuntimeError(f"âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ <section id='articles'> ÏƒÏ„Î¿ {path}")
    
    # Î•Ï…ÏÎµÏ„Î®ÏÎ¹Î¿: href -> Ï„Î¿ div container Ï„Î¿Ï…
    existing = {}
    for div in section.find_all("div", class_="link-container"):
        a_tag = div.find("a", href=True)
        if a_tag:
            existing[a_tag["href"]] = div
            
    return soup, section, existing

# =========================
# MAIN PROCESS
# =========================
def run_update():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dry-run", action="store_true")
    args = parser.parse_args()

    # 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Archives
    archives_data = {}
    for lang, path in ARCHIVES.items():
        soup, section, existing = load_archive_soup(path)
        archives_data[lang] = {"soup": soup, "section": section, "existing": existing, "path": path}

    # 2. ÎšÎ‘Î˜Î‘Î¡Î™Î£ÎœÎŸÎ£ ÎŸÎ¡Î¦Î‘ÎÎ©Î (Î”Î¹Î±Î³ÏÎ±Ï†Î® Î±Î½ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Ï€Î¹Î±)
    log("ğŸ§¹ Checking for orphan links...")
    for lang, data in archives_data.items():
        to_remove = []
        for href, div in data["existing"].items():
            # ÎœÎ·Î½ Î´Î¹Î±Î³ÏÎ¬Ï†ÎµÎ¹Ï‚ Ï€Î¿Ï„Î­ Ï„Î± index
            if "index" in href: continue
            
            path_in_root = os.path.join(ROOT_DIR, href)
            path_in_new = os.path.join(NEW_HTML_DIR, href)
            
            # Î‘Î½ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Ï€Î¿Ï…Î¸ÎµÎ½Î¬, Î´Î¹Î­Î³ÏÎ±ÏˆÎµ Ï„Î¿ Î±Ï€ÏŒ Ï„Î¿ Archive
            if not os.path.exists(path_in_root) and not os.path.exists(path_in_new):
                div.decompose() # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· Î±Ï€ÏŒ Ï„Î¿ HTML tree
                to_remove.append(href)
        
        for r in to_remove:
            log(f"ğŸ—‘ï¸ Removed from {lang} archive: {r} (File not found)")
            del data["existing"][r]

    # 3. Î•Î Î•ÎÎ•Î¡Î“Î‘Î£Î™Î‘ ÎÎ•Î©Î Î‘Î¡Î§Î•Î™Î©Î
    for file in os.listdir(NEW_HTML_DIR):
        if not file.endswith(".html") or "archives" in file or "template" in file:
            continue
        
        full_path = os.path.join(NEW_HTML_DIR, file)
        title, keywords, href, lang, date_obj = extract_html_metadata(full_path)
        
        if lang not in archives_data: continue
        
        data = archives_data[lang]
        if href in data["existing"]:
            log(f"â­ï¸ Skipping {file} (Already in archive)")
            continue

        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î½Î­Î¿Ï… Container
        icon = extract_icon(title)
        display_title = clean_title(title)
        display_date = format_date(date_obj, lang)
        data_date = date_obj.strftime("%Y-%m-%d")

        new_div = BeautifulSoup(f'''
<div class="link-container" data-title="{display_title}" data-date="{data_date}" data-tags="{keywords}">
  <a href="{href}" target="_blank" class="link-button">
    <span class="icon">{icon}</span>{display_title}
  </a>
  <span class="pub-date">{display_date}</span>
</div>
''', "html.parser")

        data["section"].insert(0, new_div)
        log(f"âœ¨ Added to {lang}: {display_title}")

    # 4. Î‘Î ÎŸÎ˜Î—ÎšÎ•Î¥Î£Î— & ÎœÎ•Î¤Î‘Î¦ÎŸÎ¡Î‘
    if not args.dry_run:
        for lang, data in archives_data.items():
            with open(data["path"], "w", encoding="utf-8") as f:
                f.write(data["soup"].prettify())
            log(f"âœ… Saved Archive ({lang})")

        # ÎœÎµÏ„Î±Ï†Î¿ÏÎ¬ ÏƒÏ„Î¿ Root
        for file in os.listdir(NEW_HTML_DIR):
            if file.endswith(".html") and "archives" not in file:
                src = os.path.join(NEW_HTML_DIR, file)
                dst = os.path.join(ROOT_DIR, file)
                shutil.move(src, dst)
        log("ğŸšš Files moved to Root folder.")

if __name__ == "__main__":
    run_update()
    log("ğŸ¯ Process Finished!")
